{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 1,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from numba import njit\n",
    "from copy import deepcopy\n",
    "from Levenshtein import distance as levenshtein_distance \n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "\n",
    "\n",
    "def read_arduino(ser,inputBufferSize):\n",
    "#     data = ser.readline((inputBufferSize+1)*2)\n",
    "    data = ser.read((inputBufferSize+1)*2)\n",
    "    out =[(int(data[i])) for i in range(0,len(data))]\n",
    "    return out\n",
    "\n",
    "def process_data(data):\n",
    "    data_in = np.array(data)\n",
    "    result = []\n",
    "    i = 1\n",
    "    while i < len(data_in)-1:\n",
    "        if data_in[i] > 127:\n",
    "            # Found beginning of frame\n",
    "            # Extract one sample from 2 bytes\n",
    "            intout = (np.bitwise_and(data_in[i],127))*128\n",
    "            i = i + 1\n",
    "            intout = intout + data_in[i]\n",
    "            result = np.append(result,intout)\n",
    "        i=i+1\n",
    "    return np.flip(np.array(result)-512)\n",
    "\n",
    "\n",
    "\n",
    "# ser.read works by waiting for <inputBufferSize> bytes from the port\n",
    "\n",
    "def read_arduinbro(wav_array, inputBufferSize, k):\n",
    "#    data = ser.readline(inputBufferSize)\n",
    "    if inputBufferSize*(k+1) < len(wav_array):\n",
    "        data = wav_array[(inputBufferSize*(k)):(inputBufferSize*(k+1))]\n",
    "    else:\n",
    "        data = wav_array[(inputBufferSize*(k))::]\n",
    "    return np.flip(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 2,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use this to find ports\n",
    "# from serial.tools import list_ports\n",
    "\n",
    "# ports = list_ports.comports()\n",
    "# for port in ports:\n",
    "#     print(port)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# # Read example data\n",
    "# baudrate = 230400\n",
    "# cport = '/dev/cu.usbmodem142301'  # set the correct port before you run it\n",
    "# ser = serial.Serial(port=cport, baudrate=baudrate)   \n",
    "\n",
    "# inputBufferSize = 10000 # 20000 = 1 second\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 3,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "['L', 'R', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'L', 'R', 'L', 'L', 'L', 'R', 'R', 'R', 'R', 'L', 'L', 'R', 'L', 'L', 'R']\n",
      "10000\n",
      "['R', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'L']\n",
      "10000\n",
      "['L', 'L', 'R', 'R', 'R', 'R', 'L', 'R', 'R', 'R', 'R', 'R', 'L', 'L', 'R', 'R']\n",
=======
      "['L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R']\n",
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
      "10000\n",
      "10000\n",
<<<<<<< HEAD
      "10000\n",
      "10000\n",
      "['L', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'L', 'R', 'R', 'R', 'L', 'L']\n",
      "['L', 'L', 'L', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'R', 'R', 'R', 'L', 'R', 'L', 'R', 'R']\n",
      "['L', 'L', 'R', 'R', 'R', 'R', 'L', 'R', 'L', 'L', 'R', 'R', 'R', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'R']\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "['R', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'R', 'R', 'L', 'R']\n",
=======
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
      "['L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'L']\n",
      "['L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R']\n",
      "['R', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "['L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L']\n",
      "['R', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R']\n",
      "['R', 'R', 'L', 'L', 'R', 'L', 'R', 'R', 'L', 'R', 'R', 'L', 'L', 'R', 'R']\n",
      "['R', 'L', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'R', 'L']\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "['R', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'L']\n",
      "10000\n",
      "['R', 'L', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'R', 'L']\n",
      "10000\n",
      "['R', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R']\n",
      "10000\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "['L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R']\n",
      "['R', 'R', 'R', 'L', 'L', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'L', 'R', 'L', 'R']\n",
      "10000\n",
      "['L', 'L', 'R', 'R', 'L', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'L', 'L', 'R', 'L', 'R', 'R']\n",
      "['L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'L', 'L', 'R', 'R', 'R', 'L', 'L']\n",
      "10000\n",
<<<<<<< HEAD
      "dict_keys(['left-middle-right-middle-eyebrow-marina', 'left-right-middle-sandeep', 'left-right-middle-marina', 'fast-left-middle-right-steph2', 'left-middle-right-middle-eyebrow-marina2', 'left-middle-right-middle-eyebrow-marina3', 'left-middle-right-middle-eyebrow-marina4', 'left-middle-right-middle', 'slow-left-middle-right-steph', 'left-middle-right-middle-eyebrow-marina5', 'left-middle', 'left-right-middle-marina3', 'left-middle-right-steph', 'left-right-middle-marina2', 'right-middle', 'left-middle-right-different-distances-steph', 'left-middle-right-steph2', 'left-middle-right-middle#2', 'fast-left-middle-right-steph']) dict_keys(['left-right-middle-sandeep', 'left-right-middle-marina', 'left-middle-right-middle-eyebrow-marina', 'fast-left-middle-right-steph2', 'left-middle-right-middle-eyebrow-marina2', 'left-middle-right-middle-eyebrow-marina3', 'left-middle-right-middle-eyebrow-marina4', 'left-middle-right-middle', 'slow-left-middle-right-steph', 'left-middle-right-middle-eyebrow-marina5', 'left-middle', 'left-right-middle-marina3', 'left-middle-right-steph', 'left-right-middle-marina2', 'right-middle', 'left-middle-right-different-distances-steph', 'left-middle-right-steph2', 'fast-left-middle-right-steph', 'left-middle-right-middle#2'])\n",
      "set()\n",
      "set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-4950ee99de34>:21: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  labels_dat = pd.read_csv(path+file, sep=\",\\t\", skiprows=1)\n"
=======
      "dict_keys(['left-middle-right-different-distances-steph', 'left-middle-right-middle#2', 'left-middle-right-middle', 'left-middle-right-steph', 'left-middle-right-steph2', 'left-middle', 'left-right-middle-marina', 'left-right-middle-marina2', 'left-right-middle-marina3', 'right-middle', 'slow-left-middle-right-steph']) dict_keys(['left-middle-right-different-distances-steph', 'left-middle-right-middle#2', 'left-middle-right-middle', 'left-middle-right-steph', 'left-middle-right-steph2', 'left-middle', 'left-right-middle-marina', 'left-right-middle-marina2', 'left-right-middle-marina3', 'right-middle', 'slow-left-middle-right-steph'])\n",
      "set()\n",
      "set()\n",
      "<ipython-input-3-9b685a735564>:21: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  labels_dat = pd.read_csv(path+file, sep=\",\\t\", skiprows=1)\n",
      "<ipython-input-3-9b685a735564>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels_dat.label[i] = \"L\"\n",
      "C:\\Users\\darap\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "<ipython-input-3-9b685a735564>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels_dat.label[i] = \"R\"\n"
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
     ]
    }
   ],
   "source": [
    "# Load training set\n",
    "\n",
    "import os\n",
    "\n",
    "path = \"Datasets/Good Data - Sandeep no errors/\"\n",
    "\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "waves = {}\n",
    "labels = {}\n",
    "\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    if (\"right\" in file.lower() or \"left\" in file.lower()) and \"blink\" not in file.lower() and \"brow\" not in file.lower() and \"fast-left-middle-right-steph\" not in file.lower() and \"left-right-middle-sandeep\" not in file.lower():\n",
    "        if file[-4::] == \".wav\":\n",
    "            samprate, wav_array = wavfile.read(path+file)\n",
    "            print(samprate)\n",
    "            waves[file[:-4]] = wav_array\n",
    "        elif file[-4::] == \".txt\":\n",
    "            labels_dat = pd.read_csv(path+file, sep=\",\\t\", skiprows=1)\n",
    "            labels_dat.columns = [\"label\", \"time\"]\n",
    "            # Change depending on whether L is coded as 1 or as 2\n",
    "            i = 0\n",
    "            while i < len(labels_dat.label):\n",
    "                if labels_dat.label[i] == 1:\n",
    "                    labels_dat.label[i] = \"L\"\n",
    "                elif labels_dat.label[i] == 2:\n",
    "                    labels_dat.label[i] = \"R\"\n",
    "                \"\"\"\n",
    "                elif labels_dat.label[i] == 3:\n",
    "                    labels_dat.label[i] = \"E\"\n",
    "                \"\"\"\n",
    "                i += 1\n",
    "\n",
    "            labels[file[:-4].replace(\".\", \"\")] = labels_dat\n",
    "            print(list(labels[file[:-4].replace(\".\", \"\")].label))\n",
    "\n",
    "print(waves.keys(), labels.keys())\n",
    "\n",
    "\n",
    "print(set(waves.keys()).difference(set(labels.keys())))\n",
    "print(set(labels.keys()).difference(set(waves.keys())))\n",
    "\n",
    "assert set(waves.keys()).difference(set(labels.keys())) == set() and set(labels.keys()).difference(set(waves.keys())) == set()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 4,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate stream\n",
    "\n",
    "# with wave.open('/Users/billydodds/Documents/Uni/DATA3888/Aqua10/Spiker_box_Louis/Short/LLL_L2.wav','r') as wav:\n",
    "#     framerate = wav.getframerate()\n",
    "#     nframes = wav.getnframes()\n",
    "samprate = 10000.0\n",
    "#     wav_array = np.array(struct.unpack(\"<\" + \"h\"*nframes, wav.readframes(nframes)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 5,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "\n",
    "def smoothing_classifier(arr, downsample_rate=10, window_size_seconds=0.3):\n",
    "    arr_ds = arr[0::downsample_rate]\n",
    "    \n",
    "    # Smooth wave\n",
    "    window_length = int(window_size_seconds*samprate/downsample_rate + 1)\n",
    "\n",
    "    #start = time.time()\n",
    "    filtered_arr = signal.savgol_filter(arr_ds, window_length, 1)\n",
    "    #end = time.time()\n",
    "    #print(\"Sav-Gol took:\", end - start, \"seconds\")\n",
    "\n",
    "    # Get max min\n",
    "    max_locs = np.array(signal.argrelextrema(filtered_arr, np.greater)[0])\n",
    "    min_locs = np.array(signal.argrelextrema(filtered_arr, np.less)[0])\n",
    "    \n",
    "    \n",
    "#     mins = arr_ds[min_locs]\n",
    "#     mins = arr_ds[min_locs]\n",
    "    \n",
    "    \n",
    "    max_min_locs = np.append(max_locs, min_locs)\n",
    "    max_min = arr_ds[max_min_locs]\n",
    "\n",
    "    # Sort vals    \n",
    "    top_3 = sorted(max_min, key=abs, reverse=True)[0:3]\n",
    "\n",
    "    if np.sum(np.sign(top_3)) == 1:\n",
    "        return \"L\"\n",
    "    elif np.sum(np.sign(top_3)) == -1:\n",
    "        return \"R\"\n",
    "    else:\n",
    "        return \"_\"\n",
    "    \n",
    "    \n",
    "    \n",
    "def smoothing_classifier(arr, downsample_rate=10, window_size_seconds=0.3, max_loops=10):\n",
    "    arr_ds = arr[0::downsample_rate]\n",
    "    \n",
    "    fs = samprate/downsample_rate\n",
    "    dt = 1/fs\n",
    "    t = np.arange(0, (len(arr_ds)*dt), dt)\n",
    "\n",
    "    # Smooth wave\n",
    "    window_length = int(window_size_seconds*samprate/downsample_rate + 1)\n",
    "\n",
    "    #start = time.time()\n",
    "    filtered_arr = signal.savgol_filter(arr_ds, window_length, 1)\n",
    "    #end = time.time()\n",
    "    #print(\"Sav-Gol took:\", end - start, \"seconds\")\n",
    "\n",
    "    # Indices of positive maxima\n",
    "    max_locs = np.array(signal.argrelextrema(filtered_arr, np.greater)[0])\n",
    "    max_vals = filtered_arr[max_locs]\n",
    "    max_locs = max_locs[max_vals > 0]\n",
    "    \n",
    "    # Indices of negative minima\n",
    "    min_locs = np.array(signal.argrelextrema(filtered_arr, np.less)[0])\n",
    "    min_vals = filtered_arr[min_locs]\n",
    "    min_locs = min_locs[min_vals < 0]\n",
    "    \n",
    "    # Appended indices\n",
    "    max_min_locs = np.append(max_locs, min_locs)\n",
    "    \n",
    "    # Values of above indices\n",
    "    max_min_values = filtered_arr[max_min_locs]\n",
    "    # Absolute value of those values\n",
    "    abs_max_min_values = np.abs(max_min_values)\n",
    "    \n",
    "    # A vector with a length equal to the number of minimums: all '-1' to say minimum\n",
    "    numMin = [-1]*len(min_locs)\n",
    "    # A vector with a length equal to the number of maximums: all '1' to say maximum\n",
    "    numMax = [1]*len(max_locs)\n",
    "    \n",
    "    # Vector same size as max_min_values with first half being maximums and second half being minimums\n",
    "    isMin = np.append(numMax, numMin)\n",
    "    \n",
    "    # Stack the three vectors\n",
    "    val_and_idx = np.vstack([abs_max_min_values, max_min_locs, isMin])\n",
    "    \n",
    "    # Sort the magnitudes of the extrema in descending order (-1 indicates descending)\n",
    "    val_and_idx_sorted = val_and_idx[ :, (-1*val_and_idx[0]).argsort()]\n",
    "    \n",
    "\n",
    "    \n",
    "    classificationFound = False\n",
    "    \n",
    "    # We will continue looping until we have an appropriate classification. This relies on having the extrema INTERCHANGE between max and min (no two min right next to eachother)\n",
    "    loops = 0\n",
    "    while not classificationFound and loops < max_loops:\n",
    "        \n",
    "        # Take the top three magnitudes\n",
    "        top_3 = val_and_idx_sorted[:, 0:3]\n",
    "        \n",
    "        # Sort according to the indices of those values\n",
    "        top_3_sorted = top_3[ :, top_3[1].argsort()]\n",
    "        \n",
    "        # Break if we run out of turning points\n",
    "        if top_3_sorted.shape != (3, 3):\n",
    "            return \"_\"\n",
    "        \n",
    "        # If two min or two max occur one after the other, we know we have an inappropriate result so we delete one of those doubled min/max\n",
    "        if top_3_sorted[2, 0]*top_3_sorted[2, 1] > 0:\n",
    "            val_and_idx_sorted = np.delete(val_and_idx_sorted, 1, 1)\n",
    "        elif top_3_sorted[2, 1]*top_3_sorted[2, 2] > 0:\n",
    "            val_and_idx_sorted = np.delete(val_and_idx_sorted, 2, 1)\n",
    "        else:\n",
    "            classificationFound = True\n",
    "        \n",
    "        loops += 1\n",
    "    \n",
    "    if np.sum(top_3_sorted[2, :]) == -1:\n",
    "        return 'L'\n",
    "    elif np.sum(top_3_sorted[2, :]) == 1:\n",
    "        return 'R'\n",
    "    else:\n",
    "        return \"_\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_pronged_smoothing_classifier(arr, downsample_rate=10, window_size_seconds=0.3, max_loops=10):\n",
    "    arr_ds = arr[0::downsample_rate]\n",
    "    \n",
    "    fs = samprate/downsample_rate\n",
    "    dt = 1/fs\n",
    "    t = np.arange(0, (len(arr_ds)*dt), dt)\n",
    "\n",
    "    # Smooth wave\n",
    "    window_length = int(window_size_seconds*samprate/downsample_rate + 1)\n",
    "    \n",
    "    #start = time.time()\n",
    "    filtered_arr = signal.savgol_filter(arr_ds, window_length, 1)\n",
    "    #end = time.time()\n",
    "    #print(\"Sav-Gol took:\", end - start, \"seconds\")\n",
    "\n",
    "\n",
    "    # Indices of positive maxima\n",
    "    max_locs = np.array(signal.argrelextrema(filtered_arr, np.greater)[0])\n",
    "    max_vals = filtered_arr[max_locs]\n",
    "    max_locs = max_locs[max_vals > 0]\n",
    "    \n",
    "    # Indices of negative minima\n",
    "    min_locs = np.array(signal.argrelextrema(filtered_arr, np.less)[0])\n",
    "    min_vals = filtered_arr[min_locs]\n",
    "    min_locs = min_locs[min_vals < 0]\n",
    "    \n",
    "    # Appended indices\n",
    "    max_min_locs = np.append(max_locs, min_locs)\n",
    "    \n",
    "    # Values of above indices\n",
    "    max_min_values = filtered_arr[max_min_locs]\n",
    "    # Absolute value of those values\n",
    "    abs_max_min_values = np.abs(max_min_values)\n",
    "    \n",
    "    # A vector with a length equal to the number of minimums: all '-1' to say minimum\n",
    "    numMin = [-1]*len(min_locs)\n",
    "    # A vector with a length equal to the number of maximums: all '1' to say maximum\n",
    "    numMax = [1]*len(max_locs)\n",
    "    \n",
    "    # Vector same size as max_min_values with first half being maximums and second half being minimums\n",
    "    isMin = np.append(numMax, numMin)\n",
    "    \n",
    "    # Stack the three vectors\n",
    "    val_and_idx = np.vstack([abs_max_min_values, max_min_locs, isMin])\n",
    "    \n",
    "    # Sort the magnitudes of the extrema in descending order (-1 indicates descending)\n",
    "    val_and_idx_sorted = val_and_idx[ :, (-1*val_and_idx[0]).argsort()]\n",
    "    \n",
    "\n",
    "    \n",
    "    classificationFound = False\n",
    "    \n",
    "    # We will continue looping until we have an appropriate classification. This relies on having the extrema INTERCHANGE between max and min (no two min right next to eachother)\n",
    "    loops = 0\n",
    "    while not classificationFound and loops < max_loops:\n",
    "        \n",
    "        # Take the top three magnitudes\n",
    "        top_3 = val_and_idx_sorted[:, 0:3]\n",
    "        \n",
    "        # Sort according to the indices of those values\n",
    "        top_3_sorted = top_3[ :, top_3[1].argsort()]\n",
    "        \n",
    "        # Break if we run out of turning points\n",
    "        if top_3_sorted.shape != (3, 3):\n",
    "            return \"_\"\n",
    "        \n",
    "        # If two min or two max occur one after the other, we know we have an inappropriate result so we delete one of those doubled min/max\n",
    "        if top_3_sorted[2, 0]*top_3_sorted[2, 1] > 0:\n",
    "            val_and_idx_sorted = np.delete(val_and_idx_sorted, 1, 1)\n",
    "        elif top_3_sorted[2, 1]*top_3_sorted[2, 2] > 0:\n",
    "            val_and_idx_sorted = np.delete(val_and_idx_sorted, 2, 1)\n",
    "        else:\n",
    "            classificationFound = True\n",
    "        \n",
    "        loops += 1\n",
    "    \n",
    "    if np.sum(top_3_sorted[2, :]) == -1:\n",
    "        return 'L'\n",
    "    elif np.sum(top_3_sorted[2, :]) == 1:\n",
    "        return 'R'\n",
    "    else:\n",
    "        return \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks for x samples (after downsampling) that are consecutively positive / negative.\n",
    "#Classifies only using the first hump of the wave.\n",
    "\n",
    "\n",
    "@njit\n",
    "def zeroes_classifier(arr, downsample_rate=10, window_size_seconds=0.3, ave_height = 350):\n",
    "    arr_ds = arr[0::downsample_rate]\n",
    "    arr_sign = np.sign(arr_ds)\n",
    "\n",
    "    consec_neg = 0\n",
    "    consec_pos = 0\n",
    "    i = 0\n",
    "    while i < len(arr_sign):\n",
    "        if consec_neg == 0 and consec_pos == 0:\n",
    "            if arr_sign[i] == 1:\n",
    "                consec_pos += 1\n",
    "            if arr_sign[i] == -1:\n",
    "                consec_neg += 1\n",
    "        if consec_neg > 0:\n",
    "            if arr_sign[i] == 1:\n",
    "                consec_neg = 0\n",
    "                consec_pos = 1\n",
    "            elif arr_sign[i] == -1:\n",
    "                consec_neg += 1\n",
    "                consec_pos = 0\n",
    "            elif arr_sign[i] == 0:\n",
    "                consec_neg, consec_pos = 0, 0\n",
    "        if consec_pos > 0:\n",
    "            if arr_sign[i] == -1:\n",
    "                consec_pos = 0\n",
    "                consec_neg = 1\n",
    "            elif arr_sign[i] == 1:\n",
    "                consec_pos += 1\n",
    "                consec_neg = 0\n",
    "            elif arr_sign[i] == 0:\n",
    "                consec_neg, consec_pos = 0, 0\n",
    "\n",
    "        if consec_neg > 200:\n",
    "            if np.sum(arr_ds[i - 200: i]) / 200 < -1 * ave_height:\n",
    "                return 'L'          \n",
    "        if consec_pos > 200:\n",
    "            if np.sum(arr_ds[i - 200: i]) / 200 > ave_height:\n",
    "                return 'R'\n",
    "        i += 1\n",
    "    return '_'\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 8,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINCE THE STREAMING DATA COMES IN FROM THE RIGHT TO THE LEFT ON THE PLOT, \n",
    "# THE RESULTANT PLOT IS ACTUALLY REFLECTED ABOUT THE Y AXIS\n",
    "\n",
    "\n",
    "def streaming_classifier(\n",
    "    wav_array,\n",
    "    window_size = 1.5, # time plotted in window [s]\n",
    "    N_loops_over_window = 15, # implicitly defines buffer to be 1/x of the window\n",
    "    samprate = 10000,\n",
    "    total_time = None,  # max time\n",
    "    hyp_detection_buffer_end = 0.3, # seconds - how much time to shave off either end of the window in order to define the middle portion\n",
    "    hyp_detection_buffer_start = 0.7,\n",
    "    hyp_event_threshold = 1800, # crossings per second\n",
    "    hyp_event_history = 5,\n",
    "    hyp_consecutive_triggers = 3,\n",
    "    hyp_consecutive_reset = 1,\n",
    "    plot = False,\n",
    "    store_events = False, \n",
    "    verbose=False,\n",
    "    classifier = \"zero_crossings\"\n",
    "):\n",
    "\n",
    "    if total_time is None:\n",
    "        total_time = len(wav_array)/samprate\n",
    "    if store_events:\n",
    "        predictions_storage = []\n",
    "    \n",
    "    predictions = \"\"\n",
    "    predictions_timestamps = []\n",
    "\n",
    "    \n",
    "    # Initialise variables\n",
    "    inputBufferSize = int(window_size/N_loops_over_window * samprate)\n",
    "    N_loops =(total_time*samprate)//inputBufferSize  # len(wav_array)//inputBufferSize. Number of inputBuffers over the whole file.\n",
    "    T_acquire = inputBufferSize/samprate    # length of time that data is acquired for\n",
    "    N_loops_over_window = window_size/T_acquire    # total number of loops to cover desire time window\n",
    "    \"\"\"\n",
    "    print(\"inputBufferSize: \", inputBufferSize)\n",
    "    print(\"N_loops: \", N_loops)\n",
    "    print(\"T_acquire: \", T_acquire)\n",
    "    print(\"N_loops_over_window: \", N_loops_over_window)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise plot\n",
    "    if plot:\n",
    "        min_y = -2000 #np.min(wav_array)\n",
    "        max_y = 2000 #np.max(wav_array)\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(1,1,1)\n",
    "        plt.ion()\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "\n",
    "\n",
    "    # Hyperparameter conversions\n",
    "    hyp_detection_buffer_start_ind = int(round(hyp_detection_buffer_start * samprate))\n",
    "    hyp_detection_buffer_end_ind = int(round(hyp_detection_buffer_end * samprate))\n",
    "\n",
    "\n",
    "    event_history = np.array([False]*hyp_event_history)\n",
    "    primed = True\n",
    "\n",
    "    for k in range(0,int(N_loops)):\n",
    "        \n",
    "        # Simulate stream\n",
    "        data_temp = read_arduinbro(wav_array, inputBufferSize, k)\n",
    "\n",
    "        # Stream\n",
    "    #     data = read_arduino(ser,inputBufferSize)\n",
    "    #     data_temp = process_data(data)\n",
    "\n",
    "\n",
    "        if k < N_loops_over_window:\n",
    "            if k==0:\n",
    "                data_plot = data_temp\n",
    "            else:\n",
    "                data_plot = np.append(data_temp,data_plot)\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            data_plot = np.roll(data_plot,len(data_temp))\n",
    "            data_plot[0:len(data_temp)] = data_temp\n",
    "\n",
    "\n",
    "        ### CLASSIFIER ###\n",
    "        \n",
    "        ## EVENT DETECTION ##\n",
    "\n",
    "        interval = data_plot[hyp_detection_buffer_start_ind:-hyp_detection_buffer_end_ind] # Take middle part of window\n",
    "\n",
    "\n",
    "    #     test_stat = np.sum(interval[0:-1] * interval[1::] <= 0) # Calculate test stat (zero crossings) \n",
    "        test_stat = np.max(interval) - np.min(interval) # Calculate test stat (range) \n",
    "        test_stat = test_stat/(len(interval)/samprate) # convert to crossings per second\n",
    "\n",
    "\n",
    "        is_event = (test_stat > hyp_event_threshold) # Test threshold\n",
    "\n",
    "        ## KEEP HISTORY ##\n",
    "\n",
    "        event_history[1::] = event_history[0:-1]\n",
    "        event_history[0] = is_event\n",
    "\n",
    "\n",
    "        ## Classification\n",
    "\n",
    "        if np.all(event_history[0:hyp_consecutive_triggers]) and primed:\n",
    "            start = time.time()\n",
    "            if classifier == \"zero_crossings\":\n",
    "                prediction = zeroes_classifier(data_plot)\n",
    "            elif classifier == \"smoothing\":\n",
    "                prediction = smoothing_classifier(data_plot)\n",
    "            else:\n",
    "                print(\"Input a classifier type please.\")\n",
    "                return None\n",
    "            end = time.time()\n",
    "            print(\"Classification took: \", end - start, \"seconds.\") if verbose else None\n",
    "            \n",
    "            print(f\"CONGRATULATIONS, ITS AN {prediction}!\") if verbose else None\n",
    "\n",
    "            if store_events:\n",
    "                predictions_storage.append(data_plot)\n",
    "                \n",
    "            predictions += prediction\n",
    "            \n",
    "            \n",
    "            end_time = round(k*inputBufferSize/samprate, 2)\n",
    "            start_time = round(end_time - window_size, 2)\n",
    "            predictions_timestamps.append((start_time, end_time))\n",
    "            \n",
    "            timer = 20\n",
    "\n",
    "            primed = False\n",
    "        elif np.all(~event_history[0:hyp_consecutive_reset]):\n",
    "            primed = True\n",
    "\n",
    "\n",
    "\n",
    "        ## PLOT ###\n",
    "\n",
    "        if plot:\n",
    "            t = (min(k+1,N_loops_over_window))*inputBufferSize/samprate*np.linspace(0,1,(data_plot).size)\n",
    "            ax1.clear()\n",
    "            # Debugging Annotations\n",
    "            if np.all(event_history[0:hyp_consecutive_triggers]) and timer >0:\n",
    "                ax1.annotate(f\"ITS AN {prediction}!!!\", (window_size/2, max_y-50))\n",
    "                timer -= 1\n",
    "            \n",
    "            ax1.annotate(f\"{event_history}\", (window_size/2, max_y-70))\n",
    "            ax1.set_xlim(0, window_size)\n",
    "            ax1.set_ylim(min_y, max_y)\n",
    "            plt.xlabel('time [s]')\n",
    "            ax1.plot(t,data_plot)\n",
    "            fig.canvas.draw()    \n",
    "            plt.show()\n",
    "    \n",
    "    if store_events:\n",
    "        return predictions, predictions_timestamps, predictions_storage\n",
    "    else:\n",
    "        return predictions, predictions_timestamps\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_streaming_classifier(\n",
    "    wav_array,\n",
    "    window_size = 1.5, # time plotted in window [s]\n",
    "    N_loops_over_window = 15, # implicitly defines buffer to be 1/x of the window\n",
    "    samprate = 10000,\n",
    "    total_time = None,  # max time\n",
    "    hyp_detection_buffer_end = 0.3, # seconds - how much time to shave off either end of the window in order to define the middle portion\n",
    "    hyp_detection_buffer_start = 0.7,\n",
    "    hyp_event_threshold = 1800, # crossings per second\n",
    "    hyp_event_history = 5,\n",
    "    hyp_consecutive_triggers = 3,\n",
    "    hyp_consecutive_reset = 1,\n",
    "    plot = False,\n",
    "    store_events = False, \n",
    "    verbose=False\n",
    "):\n",
    "    \n",
    "    #print(\"new streaming classifier!\")\n",
    "    if total_time is None:\n",
    "        total_time = len(wav_array)/samprate\n",
    "    if store_events:\n",
    "        predictions_storage = []\n",
    "    \n",
    "    predictions = \"\"\n",
    "    predictions_timestamps = []\n",
    "\n",
    "    \n",
    "    # Initialise variables\n",
    "    inputBufferSize = int(window_size/N_loops_over_window * samprate)\n",
    "    N_loops = (total_time*samprate)//inputBufferSize  # len(wav_array)//inputBufferSize \n",
    "    T_acquire = inputBufferSize/samprate    # length of time that data is acquired for \n",
    "    N_loops_over_window = window_size/T_acquire    # total number of loops to cover desire time window\n",
    "\n",
    "\n",
    "    # Initialise plot\n",
    "    if plot:\n",
    "        min_y = -2000 #np.min(wav_array)\n",
    "        max_y = 2000 #np.max(wav_array)\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(1,1,1)\n",
    "        plt.ion()\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "\n",
    "\n",
    "    # Hyperparameter conversions\n",
    "    hyp_detection_buffer_start_ind = int(round(hyp_detection_buffer_start * samprate))\n",
    "    hyp_detection_buffer_end_ind = int(round(hyp_detection_buffer_end * samprate))\n",
    "\n",
    "\n",
    "    event_history = np.array([False]*hyp_event_history)\n",
    "    primed = True\n",
    "\n",
    "    for k in range(0,int(N_loops)):\n",
    "        \n",
    "        # Simulate stream\n",
    "        data_temp = read_arduinbro(wav_array, inputBufferSize, k)\n",
    "\n",
    "        # Stream\n",
    "    #     data = read_arduino(ser,inputBufferSize)\n",
    "    #     data_temp = process_data(data)\n",
    "\n",
    "\n",
    "        if k < N_loops_over_window:\n",
    "            if k==0:\n",
    "                data_plot = data_temp\n",
    "            else:\n",
    "                data_plot = np.append(data_temp,data_plot)\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            data_plot = np.roll(data_plot,len(data_temp))\n",
    "            data_plot[0:len(data_temp)] = data_temp\n",
    "\n",
    "\n",
    "        ### CLASSIFIER ###\n",
    "        \n",
    "        ## EVENT DETECTION ##\n",
    "\n",
    "        interval = data_plot[hyp_detection_buffer_start_ind:-hyp_detection_buffer_end_ind] # Take middle part of window\n",
    "\n",
    "\n",
    "    #     test_stat = np.sum(interval[0:-1] * interval[1::] <= 0) # Calculate test stat (zero crossings) \n",
    "        test_stat = np.max(interval) - np.min(interval) # Calculate test stat (range) \n",
    "        test_stat = test_stat/(len(interval)/samprate) # convert to crossings per second\n",
    "\n",
    "\n",
    "        is_event = (test_stat > hyp_event_threshold) # Test threshold\n",
    "\n",
    "        ## KEEP HISTORY ##\n",
    "\n",
    "        event_history[1::] = event_history[0:-1]\n",
    "        event_history[0] = is_event\n",
    "\n",
    "\n",
    "        ## Classification\n",
    "\n",
    "        if np.all(event_history[0:hyp_consecutive_triggers]) and primed:\n",
    "            start = time.time()\n",
    "            prediction = zeroes_classifier(data_plot)\n",
    "            end = time.time()\n",
    "            print(\"Classification took: \", end - start, \"seconds.\") if verbose else None\n",
    "            \n",
    "            print(f\"CONGRATULATIONS, ITS AN {prediction}!\") if verbose else None\n",
    "\n",
    "            if store_events:\n",
    "                predictions_storage.append(data_plot)\n",
    "                \n",
    "            predictions += prediction\n",
    "            \n",
    "            \n",
    "            end_time = round(k*inputBufferSize/samprate, 2)\n",
    "            start_time = round(end_time - window_size, 2)\n",
    "            predictions_timestamps.append((start_time, end_time))\n",
    "            \n",
    "            timer = 20\n",
    "\n",
    "            primed = False\n",
    "        elif np.all(~event_history[0:hyp_consecutive_reset]):\n",
    "            primed = True\n",
    "\n",
    "\n",
    "\n",
    "        ## PLOT ###\n",
    "\n",
    "        if plot:\n",
    "            t = (min(k+1,N_loops_over_window))*inputBufferSize/samprate*np.linspace(0,1,(data_plot).size)\n",
    "            ax1.clear()\n",
    "            # Debugging Annotations\n",
    "            if np.all(event_history[0:hyp_consecutive_triggers]) and timer >0:\n",
    "                ax1.annotate(f\"ITS AN {prediction}!!!\", (window_size/2, max_y-50))\n",
    "                timer -= 1\n",
    "            \n",
    "            ax1.annotate(f\"{event_history}\", (window_size/2, max_y-70))\n",
    "            ax1.set_xlim(0, window_size)\n",
    "            ax1.set_ylim(min_y, max_y)\n",
    "            plt.xlabel('time [s]')\n",
    "            ax1.plot(t,data_plot)\n",
    "            fig.canvas.draw()    \n",
    "            plt.show()\n",
    "    \n",
    "    if store_events:\n",
    "        return predictions, predictions_timestamps, predictions_storage\n",
    "    else:\n",
    "        return predictions, predictions_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
=======
   "execution_count": 9,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(wav_array, labels_dat, predictions, predictions_timestamps, ax, i,\n",
    "                     title=\"\", before_buffer = 0.7, after_buffer = 1, actual_alpha=0.2,\n",
    "                     wave_alpha=1, pred_alpha = 0.5):\n",
    "    \n",
    "    time_seq = np.linspace(1, len(wav_array), len(wav_array))/samprate\n",
    "\n",
    "    \n",
    "\n",
    "    left_events_bool = np.array([False]*len(time_seq))\n",
    "    for time in labels_dat.time[labels_dat.label == \"L\"]:\n",
    "        left_events_bool = ( (time_seq > time - before_buffer) & (time_seq < time+after_buffer) ) | left_events_bool\n",
    "\n",
    "    right_events_bool = np.array([False]*len(time_seq))\n",
    "    for time in labels_dat.time[labels_dat.label == \"R\"]:\n",
    "        right_events_bool = ( (time_seq > time - before_buffer) & (time_seq < time+after_buffer) ) | right_events_bool\n",
    "        \n",
    "    left_preds_bool = np.array([False]*len(time_seq))\n",
    "    right_preds_bool = np.array([False]*len(time_seq))\n",
    "    idk_preds_bool = np.array([False]*len(time_seq))\n",
    "    for pred, times in zip(predictions, predictions_timestamps):\n",
    "        if pred == \"L\":\n",
    "            left_preds_bool = ( (time_seq > times[0]) & (time_seq < times[1]) ) | left_preds_bool\n",
    "        elif pred == \"R\":\n",
    "            right_preds_bool = ( (time_seq > times[0]) & (time_seq < times[1]) ) | right_preds_bool\n",
    "        else:\n",
    "            idk_preds_bool = ( (time_seq > times[0]) & (time_seq < times[1]) ) | idk_preds_bool\n",
    "            \n",
    "\n",
    "    ax[i].plot(time_seq, wav_array, alpha=wave_alpha)\n",
    "\n",
    "    \n",
    "    # Plot actuals\n",
    "    ax[i].fill_between(time_seq, 2500, -2500,\n",
    "                     where = left_events_bool,\n",
    "                     color = 'g',\n",
    "                     label = \"L\",\n",
    "                     alpha=actual_alpha)\n",
    "\n",
    "    ax[i].fill_between(time_seq, 2500, -2500,\n",
    "                     where = right_events_bool,\n",
    "                     color = 'r',\n",
    "                     label = \"R\",\n",
    "                     alpha=actual_alpha)\n",
    "    \n",
    "    # Plot predictions\n",
    "    ax[i].fill_between(time_seq, 2500, -2500,\n",
    "                     where = left_preds_bool,\n",
    "                     color = 'g',\n",
    "                     label = \"Pred L\",\n",
    "                     alpha=pred_alpha)\n",
    "\n",
    "    ax[i].fill_between(time_seq, 2500, -2500,\n",
    "                     where = right_preds_bool,\n",
    "                     color = 'r',\n",
    "                     label = \"Pred R\",\n",
    "                     alpha=pred_alpha)\n",
    "    \n",
    "    ax[i].fill_between(time_seq, 2500, -2500,\n",
    "                     where = idk_preds_bool,\n",
    "                     color = 'y',\n",
    "                     label = \"Pred idk\",\n",
    "                     alpha=pred_alpha)\n",
    "    \n",
    "    ax[i].set_title(key)\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 39,
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "inputBufferSize:  1000\n",
      "N_loops:  549.0\n",
      "T_acquire:  0.1\n",
      "N_loops_over_window:  20.0\n",
      "File:  left-middle-right-middle-eyebrow-marina\n",
      "False negative triggers: 0 []\n",
      "False positive triggers: 1 [(35.1, 37.1)]\n",
      "inputBufferSize:  1000\n",
      "N_loops:  579.0\n",
      "T_acquire:  0.1\n",
      "N_loops_over_window:  20.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
=======
      "SUMMARY OF 3-PRONGED SMOOTHING CLASSIFIER:\n--------------------------\nTime taken:  0.858168363571167\nAverage efficiency (seconds):  0.003884801865128863\nSum of levenshtein distances:  59\nTotal number of labelled characters (sum of all files):  220\n"
     ]
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
    }
   ],
   "source": [
    "#SMOOTHING CLASSIFIER\n",
    "\n",
    "from copy import deepcopy\n",
    "from Levenshtein import distance as levenshtein_distance \n",
    "\n",
    "# %matplotlib notebook\n",
    "big_start = time.time()\n",
    "before_buffer = 0.7\n",
    "after_buffer = 1\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(len(waves), 1)\n",
    "fig.set_size_inches(10, 3*len(waves))\n",
    "\"\"\"\n",
    "\n",
    "efficiency = []\n",
    "lev_dists = []\n",
    "actual_lengths = []     #lengths of actual strings for each wave file\n",
    "\n",
    "smoothing_3_pronged_pred = \"\"\n",
    "i=0\n",
    "for key in waves.keys():\n",
    "    start = time.time()\n",
    "    predictions, predictions_timestamps, predictions_storage = streaming_classifier(waves[key],\n",
    "                                                                                    window_size=2,\n",
    "                                                                                    N_loops_over_window = 20,\n",
    "                                                                                    hyp_detection_buffer_end = 0.1, # seconds - how much time to shave off either end of the window in order to define the middle portion\n",
    "                                                                                    hyp_detection_buffer_start = 1,\n",
    "                                                                                    hyp_event_history = 10,\n",
    "                                                                                    hyp_event_threshold = 1200,\n",
    "                                                                                    hyp_consecutive_triggers = 6,\n",
    "                                                                                    store_events=True,\n",
    "                                                                                    classifier = \"smoothing\")\n",
    "    \n",
    "    end = time.time()\n",
    "    efficiency.append((end - start) / max(len(predictions) ,len(list(labels[key].label))))\n",
    "    #print(\"streaming_classifier took: \", (end - start) / len(predictions), \"seconds per classification (\", len(predictions), \" predictions took \", end - start,\"seconds ).\")\n",
    "    lev_dists.append(levenshtein_distance(predictions, \"\".join(list(labels[key].label))))\n",
    "    actual_lengths.append(len(list(labels[key].label)))\n",
    "    #print(\"Predictions: \", predictions)\n",
    "    #print(\"Actual: \", \"\".join(list(labels[key].label)))\n",
    "    smoothing_3_pronged_pred += predictions\n",
    "    #print(list(labels[key].label))\n",
    "    \n",
    "    \"\"\"\n",
    "    plot_predictions(waves[key], labels[key], predictions, predictions_timestamps, ax, i, title=key, actual_alpha=0.2)\n",
    "    ax[i].set_xlim(-13)\n",
    "    i+=1\n",
    "    \"\"\"\n",
    "    \n",
    "    actual_times = [(time-before_buffer, time+after_buffer) for time in labels[key].time]\n",
    "    actual_leftovers = deepcopy(actual_times)\n",
    "    pred_leftovers = deepcopy(predictions_timestamps)\n",
    "    \n",
    "    \n",
    "    for act_times in actual_times:\n",
    "        for pred_times in predictions_timestamps:\n",
    "            if act_times[0] < pred_times[1] and act_times[1] > pred_times[0] and pred_times in pred_leftovers and act_times in actual_leftovers:\n",
    "                actual_leftovers.remove(act_times)\n",
    "                pred_leftovers.remove(pred_times)\n",
    "            \n",
    "    \"\"\"          \n",
    "    print(\"File: \", key)\n",
    "    print(f\"False negative triggers: {len(actual_leftovers)}\", actual_leftovers)\n",
    "    print(f\"False positive triggers: {len(pred_leftovers)}\",  pred_leftovers)\n",
    "    \"\"\"\n",
    "    #print((len(actual_leftovers) + len(pred_leftovers)) / len(labels[key].label))\n",
    "    \n",
    "big_end = time.time()\n",
    "%matplotlib inline\n",
    "print(\"SUMMARY OF 3-PRONGED SMOOTHING CLASSIFIER:\")\n",
    "print(\"--------------------------\")\n",
    "print(\"Time taken: \", big_end - big_start) \n",
    "#print(\"List of Efficiencies: \", efficiency)\n",
    "print(\"Average efficiency (seconds): \", sum(efficiency) / len(efficiency))\n",
    "#print(\"lev dists: \", lev_dists)\n",
    "print(\"Sum of levenshtein distances: \", sum(lev_dists))\n",
    "print(\"Total number of labelled characters (sum of all files): \", sum(actual_lengths))\n",
    "#print(\"Length of list of levenshtein distances: \", len(lev_dists))\n",
    "#lev_dists = pd.DataFrame(lev_dists)\n",
    "#lev_dists.boxplot()\n",
    "\n",
    "#SMOOTHING CLASSIFIER\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Levenshtein'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'"
=======
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SUMMARY OF NEW CLASSIFIER: (need to run twice for best speed)\n--------------------------\nTime taken:  0.6226174831390381\nAverage efficiency (seconds):  0.0028441273316232724\nSum of levenshtein distances:  53\nTotal number of labelled characters (sum of all files):  220\n"
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "\n",
=======
    "#ZERO-CROSSINGS CLASSIFIER\n",
    "\n",
    "from copy import deepcopy\n",
    "from Levenshtein import distance as levenshtein_distance \n",
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
    "\n",
    "# %matplotlib notebook\n",
    "big_start = time.time()\n",
    "before_buffer = 0.7\n",
    "after_buffer = 1\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(len(waves), 1)\n",
    "fig.set_size_inches(10, 3*len(waves))\n",
    "\"\"\"\n",
    "\n",
    "efficiency = []\n",
    "lev_dists = []\n",
    "actual_lengths = []     #lengths of actual strings for each wave file\n",
    "\n",
    "zero_crossings_pred = \"\"\n",
    "i=0\n",
    "for key in waves.keys():\n",
    "    start = time.time()\n",
    "    predictions, predictions_timestamps, predictions_storage = streaming_classifier(waves[key],\n",
    "                                                                                    window_size=2,\n",
    "                                                                                    N_loops_over_window = 20,\n",
    "                                                                                    hyp_detection_buffer_end = 0.1, # seconds - how much time to shave off either end of the window in order to define the middle portion\n",
    "                                                                                    hyp_detection_buffer_start = 1,\n",
    "                                                                                    hyp_event_threshold = 1200,\n",
    "                                                                                    hyp_event_history = 10,\n",
    "                                                                                    hyp_consecutive_triggers = 6,\n",
    "                                                                                    store_events=True,\n",
    "                                                                                    classifier = \"zero_crossings\")\n",
    "    \n",
    "    end = time.time()\n",
    "    efficiency.append((end - start) / max(len(predictions) ,len(list(labels[key].label))))\n",
    "    #print(\"streaming_classifier took: \", (end - start) / len(predictions), \"seconds per classification (\", len(predictions), \" predictions took \", end - start,\"seconds ).\")\n",
    "    lev_dists.append(levenshtein_distance(predictions, \"\".join(list(labels[key].label))))\n",
    "    #print(list(labels[key].label))\n",
    "    #print(\"Predictions: \", predictions)\n",
    "    #print(\"Actual: \", \"\".join(list(labels[key].label)))\n",
    "    actual_lengths.append(len(list(labels[key].label)))\n",
    "    \"\"\"\n",
    "    plot_predictions(waves[key], labels[key], predictions, predictions_timestamps, ax, i, title=key, actual_alpha=0.2)\n",
    "    ax[i].set_xlim(-13)\n",
    "    i+=1\n",
    "    \"\"\"\n",
    "    \n",
    "    actual_times = [(time-before_buffer, time+after_buffer) for time in labels[key].time]\n",
    "    actual_leftovers = deepcopy(actual_times)\n",
    "    pred_leftovers = deepcopy(predictions_timestamps)\n",
    "    \n",
    "    \n",
    "    for act_times in actual_times:\n",
    "        for pred_times in predictions_timestamps:\n",
    "            if act_times[0] < pred_times[1] and act_times[1] > pred_times[0] and pred_times in pred_leftovers and act_times in actual_leftovers:\n",
    "                actual_leftovers.remove(act_times)\n",
    "                pred_leftovers.remove(pred_times)\n",
    "    \n",
    "\n",
    "    \"\"\" \n",
    "    print(\"File: \", key)\n",
    "    print(f\"False negative triggers: {len(actual_leftovers)}\", actual_leftovers)\n",
    "    print(f\"False positive triggers: {len(pred_leftovers)}\",  pred_leftovers)\n",
    "    \"\"\"\n",
    "    #print((len(actual_leftovers) + len(pred_leftovers)) / len(labels[key].label))\n",
    "    \n",
    "big_end = time.time()\n",
    "%matplotlib inline\n",
    "print(\"SUMMARY OF NEW CLASSIFIER: (need to run twice for best speed)\")\n",
    "print(\"--------------------------\")\n",
    "print(\"Time taken: \", big_end - big_start) \n",
    "#print(\"List of Efficiencies: \", efficiency)\n",
    "print(\"Average efficiency (seconds): \", sum(efficiency) / len(efficiency))\n",
    "#print(\"lev dists: \", lev_dists)\n",
    "print(\"Sum of levenshtein distances: \", sum(lev_dists))\n",
    "print(\"Total number of labelled characters (sum of all files): \", sum(actual_lengths))\n",
    "#print(\"Length of list of levenshtein distances: \", len(lev_dists))\n",
    "#lev_dists = pd.DataFrame(lev_dists)\n",
    "#lev_dists.boxplot()\n",
    "\n",
    "#ZERO-CROSSINGS CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import IPython\n",
    "def smooth_wave(arr,t, downsample_rate=10, window_size_seconds=0.3):\n",
    "    arr_ds = arr[0::downsample_rate]\n",
    "    t_ds = t[0::downsample_rate]\n",
    "    \n",
    "    # Smooth wave\n",
    "    window_length = int(window_size_seconds*samprate/downsample_rate + 1)\n",
    "    filtered_arr = signal.savgol_filter(arr_ds, window_length, 1)\n",
    "    \n",
    "    return t_ds, filtered_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "18\n",
      "fast-left-middle-right-steph\n",
      "inputBufferSize:  1000\n",
      "N_loops:  273.0\n",
      "T_acquire:  0.1\n",
      "N_loops_over_window:  20.0\n"
=======
      "18\nleft-middle-right-different-distances-steph\n"
>>>>>>> ac3129a25f14bbf7e72834eea2baaaea19b37c9f
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-16db24b8cacf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msamprate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlinspace\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\function_base.py\u001b[0m in \u001b[0;36mlinspace\u001b[1;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \"\"\"\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of samples, %s, must be non-negative.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "print(len(predictions_storage))\n",
    "\n",
    "# fig, ax = plt.subplots(8, 3)\n",
    "\n",
    "# fig.set_size_inches(5, 15)    \n",
    "\n",
    "%matplotlib notebook\n",
    "    \n",
    "    \n",
    "i=0\n",
    "for key in waves.keys():\n",
    "    \n",
    "    print(key)\n",
    "    if key == \"left-right\":\n",
    "        continue\n",
    "    \n",
    "    window_size = 2\n",
    "    predictions, predictions_timestamps, predictions_storage = streaming_classifier(waves[key],\n",
    "                                                                                    window_size=2,\n",
    "                                                                                    N_loops_over_window = 20,\n",
    "                                                                                    hyp_detection_buffer_end = 0.1, # seconds - how much time to shave off either end of the window in order to define the middle portion\n",
    "                                                                                    hyp_detection_buffer_start = 1,\n",
    "                                                                                    hyp_event_history = 7,\n",
    "                                                                                    hyp_consecutive_triggers = 6,\n",
    "                                                                                    store_events=True,\n",
    "                                                                                    classifier = \"smoothing\")\n",
    "    \n",
    "    \n",
    "    t = np.linspace(0, window_size, window_size*samprate)\n",
    "    \n",
    "    for arr, pred, act in zip(predictions_storage, predictions, labels[key].label):\n",
    "        \n",
    "        if pred != act:\n",
    "            print(key, i)\n",
    "            t_ds, smooth_arr = smooth_wave(arr,t, downsample_rate=10, window_size_seconds=0.3)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(t*samprate, np.flip(arr))\n",
    "            plt.plot(t_ds*samprate, np.flip(smooth_arr))\n",
    "\n",
    "            plt.ylim(-2500, 2500)\n",
    "\n",
    "            plt.title(f\"Predicted: {pred}, Actual: {act}\")\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "7d845306f7225881f10ddea820c2625692dd8b44140339028ba513f9926a16c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
